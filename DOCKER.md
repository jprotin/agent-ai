# Agent IA de D√©veloppement - Docker

Guide d'installation et d'utilisation avec Docker Compose.

## üê≥ Pr√©requis

- **Docker** 20.10+ ([Installer Docker](https://docs.docker.com/get-docker/))
- **Docker Compose** 2.0+ (inclus avec Docker Desktop)
- **Minimum 8 GB RAM** (16 GB recommand√© pour les gros mod√®les)
- **10 GB d'espace disque libre** (pour les mod√®les IA)

## üöÄ D√©marrage Rapide

### Option 1 : Utiliser le script (Recommand√©)

```bash
# Rendre le script ex√©cutable (une seule fois)
chmod +x start.sh

# D√©marrer l'application
./start.sh

# L'application sera disponible sur http://localhost:3000
# Ollama API sur http://localhost:11434
```

### Option 2 : Docker Compose manuel

```bash
# D√©marrer tous les services
docker-compose up -d

# Voir les logs
docker-compose logs -f

# Arr√™ter les services
docker-compose down
```

## üìã Commandes du Script

```bash
./start.sh start           # D√©marrer tous les services
./start.sh stop            # Arr√™ter tous les services
./start.sh restart         # Red√©marrer
./start.sh logs            # Voir les logs en temps r√©el
./start.sh status          # Statut des services
./start.sh pull-model llama3  # T√©l√©charger un mod√®le
./start.sh list-models     # Lister les mod√®les install√©s
./start.sh rebuild         # Reconstruire l'application
./start.sh clean           # Tout nettoyer
./start.sh shell           # Shell dans le conteneur app
./start.sh ollama-shell    # Shell dans le conteneur ollama
```

## üèóÔ∏è Architecture Docker

### Services

1. **ollama** - Serveur Ollama pour l'IA locale
   - Port: 11434
   - Volume: `ollama_data` (persistance des mod√®les)
   
2. **ollama-setup** - T√©l√©charge automatiquement le mod√®le `codellama`
   - S'ex√©cute une fois au premier d√©marrage
   - Peut √™tre relanc√© manuellement pour d'autres mod√®les

3. **app** - Application Next.js
   - Port: 3000
   - Volume: `./output` mont√© pour acc√®s aux fichiers g√©n√©r√©s

### Volumes

- `ollama_data` : Stocke les mod√®les t√©l√©charg√©s (persistant)
- `./output` : Fichiers de code g√©n√©r√©s (bind mount)

## üîß Configuration

### Variables d'Environnement

Cr√©er un fichier `.env` √† la racine si n√©cessaire :

```env
# Port de l'application (d√©faut: 3000)
APP_PORT=3000

# Port d'Ollama (d√©faut: 11434)
OLLAMA_PORT=11434

# URL d'Ollama depuis l'application
OLLAMA_BASE_URL=http://ollama:11434
```

### Mod√®les Recommand√©s

```bash
# Mod√®le pour le code (l√©ger, 7B)
./start.sh pull-model codellama

# Mod√®le g√©n√©raliste (performant, 8B)
./start.sh pull-model llama3

# Sp√©cialiste du code (excellent, 6.7B)
./start.sh pull-model deepseek-coder

# Grand mod√®le g√©n√©raliste (33B, n√©cessite beaucoup de RAM)
./start.sh pull-model llama3:70b
```

### GPU Support (Optionnel)

Pour utiliser un GPU NVIDIA, d√©commentez dans `docker-compose.yml` :

```yaml
ollama:
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]
```

Pr√©requis : [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html)

## üìä Utilisation

1. **D√©marrer l'application**
   ```bash
   ./start.sh
   ```

2. **Ouvrir le navigateur**
   ```
   http://localhost:3000
   ```

3. **V√©rifier la connexion**
   - Le bandeau doit afficher "Connect√© √† Ollama" en vert
   - S√©lectionner le mod√®le dans le dropdown

4. **Uploader une sp√©cification**
   - Utiliser l'exemple : `examples/spec-calculatrice.md`
   - Ou cr√©er votre propre sp√©cification

5. **G√©n√©rer du code**
   - L'agent analyse automatiquement
   - R√©pondre aux questions
   - Cliquer sur "G√©n√©rer le Code"
   - Les fichiers sont dans `./output/`

## üîç Monitoring

### Logs en Temps R√©el

```bash
# Tous les services
./start.sh logs

# Un service sp√©cifique
docker-compose logs -f app
docker-compose logs -f ollama
```

### Statut des Services

```bash
./start.sh status
```

### Ressources Utilis√©es

```bash
docker stats
```

## üêõ D√©pannage

### Probl√®me : "Ollama non accessible"

**Solution 1 : V√©rifier que le conteneur fonctionne**
```bash
docker-compose ps
docker-compose logs ollama
```

**Solution 2 : Red√©marrer Ollama**
```bash
docker-compose restart ollama
sleep 10  # Attendre le d√©marrage
```

### Probl√®me : Pas de mod√®les disponibles

```bash
# V√©rifier les mod√®les install√©s
./start.sh list-models

# T√©l√©charger un mod√®le
./start.sh pull-model codellama
```

### Probl√®me : L'application ne d√©marre pas

```bash
# Voir les logs d√©taill√©s
docker-compose logs app

# Reconstruire l'image
./start.sh rebuild
./start.sh restart
```

### Probl√®me : Manque de m√©moire

```bash
# Utiliser un mod√®le plus l√©ger
./start.sh pull-model codellama:7b

# Ou augmenter la RAM allou√©e √† Docker
# Docker Desktop > Settings > Resources > Memory
```

### Probl√®me : Fichiers non g√©n√©r√©s

```bash
# V√©rifier les permissions du dossier output
ls -la output/

# Cr√©er le dossier si n√©cessaire
mkdir -p output
chmod 777 output
```

### Probl√®me : Port d√©j√† utilis√©

```bash
# Modifier le port dans docker-compose.yml
# Changer "3000:3000" par "8080:3000" par exemple

# Ou arr√™ter le service qui utilise le port
lsof -i :3000
kill -9 <PID>
```

## üßπ Maintenance

### Nettoyer les Conteneurs Arr√™t√©s

```bash
docker-compose down
```

### Nettoyer Compl√®tement (ATTENTION : supprime les mod√®les)

```bash
./start.sh clean
```

### Mettre √† Jour l'Application

```bash
# R√©cup√©rer les derni√®res modifications
git pull

# Reconstruire
./start.sh rebuild
./start.sh restart
```

### Sauvegarder les Mod√®les

```bash
# Backup du volume Ollama
docker run --rm -v ai-coding-agent_ollama_data:/data -v $(pwd):/backup \
  alpine tar czf /backup/ollama-backup.tar.gz -C /data .
```

### Restaurer les Mod√®les

```bash
# Restore du volume Ollama
docker run --rm -v ai-coding-agent_ollama_data:/data -v $(pwd):/backup \
  alpine tar xzf /backup/ollama-backup.tar.gz -C /data
```

## üìà Performance

### Temps de Chargement des Mod√®les

| Mod√®le | Taille | RAM n√©cessaire | Vitesse |
|--------|--------|----------------|---------|
| codellama:7b | ~4 GB | 8 GB | Rapide |
| llama3:8b | ~5 GB | 8 GB | Rapide |
| deepseek-coder:6.7b | ~4 GB | 8 GB | Tr√®s rapide |
| llama3:70b | ~40 GB | 64 GB | Lent |

### Optimisations

1. **Utiliser un SSD** pour les volumes Docker
2. **Allouer suffisamment de RAM** √† Docker Desktop
3. **Garder un seul mod√®le** pour √©conomiser l'espace
4. **Utiliser un GPU** si disponible pour 5-10x plus rapide

## üîê S√©curit√©

### R√©seau

Les services sont isol√©s dans un r√©seau Docker interne. Seuls les ports 3000 et 11434 sont expos√©s sur localhost.

### Volumes

Le dossier `./output` est mont√© en lecture/√©criture. Les fichiers g√©n√©r√©s y sont accessibles depuis l'h√¥te.

### Production

Pour la production, consid√©rez :
- Utiliser des secrets Docker pour les credentials
- Ajouter un reverse proxy (nginx, traefik)
- Configurer HTTPS
- Limiter les ressources avec `deploy.resources`

## üìö Ressources

- [Documentation Ollama](https://ollama.ai/docs)
- [Documentation Docker](https://docs.docker.com/)
- [Liste des mod√®les Ollama](https://ollama.ai/library)
- [Next.js Documentation](https://nextjs.org/docs)

## üÜò Support

En cas de probl√®me :
1. V√©rifier les logs : `./start.sh logs`
2. Consulter la section D√©pannage
3. Ouvrir une issue sur GitHub
4. Contact : johan@nantares.consulting
