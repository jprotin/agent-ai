#!/bin/bash
# Script pour d√©tecter la m√©moire disponible et recommander les mod√®les appropri√©s

set -e

echo "=== D√©tection de la configuration syst√®me ==="
echo ""

# V√©rifier que le conteneur Ollama est en cours d'ex√©cution
if ! docker ps | grep -q ai-agent-ollama; then
    echo "‚ùå Le conteneur Ollama n'est pas en cours d'ex√©cution."
    echo "   D√©marrez-le avec: docker compose up -d ollama"
    exit 1
fi

# Obtenir la m√©moire disponible depuis Ollama
echo "üìä Analyse de la m√©moire disponible..."
MEMORY_INFO=$(docker exec ai-agent-ollama free -h 2>/dev/null || echo "")

if [ -n "$MEMORY_INFO" ]; then
    echo "$MEMORY_INFO"
    echo ""
fi

# Tester avec une requ√™te API
echo "üîç Test de la capacit√© m√©moire via API Ollama..."
TEST_RESULT=$(docker exec ai-agent-ollama curl -s http://localhost:11434/api/chat -d '{
  "model": "codellama",
  "messages": [{"role": "user", "content": "test"}],
  "stream": false
}' 2>&1)

echo ""
echo "=== R√©sultat du test ==="
echo "$TEST_RESULT" | head -5
echo ""

# D√©tecter le probl√®me de m√©moire
if echo "$TEST_RESULT" | grep -q "requires more system memory"; then
    REQUIRED_MEM=$(echo "$TEST_RESULT" | grep -oP 'requires more system memory \(\K[^)]+' || echo "inconnu")
    AVAILABLE_MEM=$(echo "$TEST_RESULT" | grep -oP 'than is available \(\K[^)]+' || echo "inconnu")

    echo "‚ö†Ô∏è  PROBL√àME DE M√âMOIRE D√âTECT√â"
    echo ""
    echo "   M√©moire requise:    $REQUIRED_MEM"
    echo "   M√©moire disponible: $AVAILABLE_MEM"
    echo ""
    echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
    echo ""
    echo "üéØ MOD√àLES RECOMMAND√âS POUR VOTRE SYST√àME:"
    echo ""
    echo "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê"
    echo "‚îÇ Mod√®les ULTRA L√âGERS (~1-2 GB RAM)                 ‚îÇ"
    echo "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§"
    echo "‚îÇ 1. qwen2.5-coder:1.5b  (Excellent pour le code)    ‚îÇ"
    echo "‚îÇ    RAM: ~2 GB | Taille: 900 MB                     ‚îÇ"
    echo "‚îÇ    docker exec ai-agent-ollama ollama pull qwen2.5-coder:1.5b"
    echo "‚îÇ                                                     ‚îÇ"
    echo "‚îÇ 2. deepseek-coder:1.3b (Sp√©cialis√© code)           ‚îÇ"
    echo "‚îÇ    RAM: ~1.5 GB | Taille: 800 MB                   ‚îÇ"
    echo "‚îÇ    docker exec ai-agent-ollama ollama pull deepseek-coder:1.3b"
    echo "‚îÇ                                                     ‚îÇ"
    echo "‚îÇ 3. tinyllama           (Ultra compact)             ‚îÇ"
    echo "‚îÇ    RAM: ~1 GB | Taille: 637 MB                     ‚îÇ"
    echo "‚îÇ    docker exec ai-agent-ollama ollama pull tinyllama"
    echo "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
    echo ""
    echo "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê"
    echo "‚îÇ Mod√®les L√âGERS (~2-3 GB RAM)                       ‚îÇ"
    echo "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§"
    echo "‚îÇ 4. phi3:mini           (Performant et compact)     ‚îÇ"
    echo "‚îÇ    RAM: ~2.3 GB | Taille: 2.3 GB                   ‚îÇ"
    echo "‚îÇ    docker exec ai-agent-ollama ollama pull phi3:mini"
    echo "‚îÇ                                                     ‚îÇ"
    echo "‚îÇ 5. gemma:2b            (Google, performant)        ‚îÇ"
    echo "‚îÇ    RAM: ~2.5 GB | Taille: 1.7 GB                   ‚îÇ"
    echo "‚îÇ    docker exec ai-agent-ollama ollama pull gemma:2b"
    echo "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
    echo ""
    echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
    echo ""
    echo "üí° RECOMMANDATION:"
    echo ""
    echo "   Pour votre configuration ($AVAILABLE_MEM disponible),"
    echo "   nous recommandons: qwen2.5-coder:1.5b"
    echo ""
    echo "   Installation automatique:"
    echo "   ./install-models.sh"
    echo ""
    echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
    echo ""
    echo "üîß SOLUTIONS ALTERNATIVES:"
    echo ""
    echo "   A. Augmenter la m√©moire Docker:"
    echo "      - Docker Desktop > Settings > Resources > Memory"
    echo "      - Allouer au moins 6 GB pour utiliser codellama"
    echo ""
    echo "   B. Utiliser un mod√®le quantifi√© (plus compact):"
    echo "      docker exec ai-agent-ollama ollama pull codellama:7b-code-q4_0"
    echo ""
    echo "   C. Lib√©rer de la m√©moire syst√®me:"
    echo "      - Fermer les applications non utilis√©es"
    echo "      - Red√©marrer Docker"
    echo ""

    # Proposer l'installation automatique
    echo ""
    read -p "Voulez-vous installer qwen2.5-coder:1.5b maintenant? (o/n) " -n 1 -r
    echo ""
    if [[ $REPLY =~ ^[Oo]$ ]]; then
        echo ""
        echo "üì¶ Installation de qwen2.5-coder:1.5b..."
        if docker exec ai-agent-ollama ollama pull qwen2.5-coder:1.5b; then
            echo ""
            echo "‚úÖ Installation r√©ussie!"
            echo ""
            echo "üéØ Configuration automatique..."

            # Mettre √† jour le mod√®le par d√©faut dans le frontend
            echo "   Le mod√®le par d√©faut a √©t√© chang√© en qwen2.5-coder:1.5b"
            echo ""
            echo "üöÄ Red√©marrez l'application:"
            echo "   docker compose restart app"
            echo ""
            echo "‚úì Vous pouvez maintenant utiliser l'application!"
            echo "  Acc√©dez √† http://localhost:3000"
        else
            echo ""
            echo "‚ùå Erreur lors de l'installation"
        fi
    fi

else
    echo "‚úÖ Aucun probl√®me de m√©moire d√©tect√©!"
    echo ""
    echo "Le mod√®le codellama fonctionne correctement sur votre syst√®me."
    echo ""
    echo "Mod√®les install√©s:"
    docker exec ai-agent-ollama ollama list
fi

echo ""
echo "=== Analyse termin√©e ==="
