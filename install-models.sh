#!/bin/bash
# Script pour installer les mod√®les Ollama n√©cessaires

set -e

echo "=== Installation des mod√®les Ollama ==="
echo ""

# V√©rifier que le conteneur Ollama est en cours d'ex√©cution
if ! docker ps | grep -q ai-agent-ollama; then
    echo "‚ùå Le conteneur Ollama n'est pas en cours d'ex√©cution."
    echo "   D√©marrez-le avec: docker compose up -d ollama"
    exit 1
fi

echo "‚úì Conteneur Ollama en cours d'ex√©cution"
echo ""

# Attendre qu'Ollama soit pr√™t
echo "Attente du d√©marrage d'Ollama..."
for i in {1..30}; do
    if docker exec ai-agent-ollama curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
        echo "‚úì Ollama est pr√™t!"
        break
    fi
    if [ $i -eq 30 ]; then
        echo "‚ùå Timeout: Ollama n'a pas d√©marr√© dans les temps"
        exit 1
    fi
    echo "  Tentative $i/30..."
    sleep 2
done

echo ""

# Lister les mod√®les existants
echo "Mod√®les actuellement install√©s:"
docker exec ai-agent-ollama ollama list

echo ""
echo "=== Installation des mod√®les recommand√©s ==="
echo ""

# Fonction pour installer un mod√®le
install_model() {
    local model=$1
    echo "üì¶ Installation du mod√®le: $model"
    if docker exec ai-agent-ollama ollama pull "$model"; then
        echo "‚úì Mod√®le $model install√© avec succ√®s!"
    else
        echo "‚ùå Erreur lors de l'installation de $model"
        return 1
    fi
    echo ""
}

# Proposer l'installation de diff√©rents mod√®les
echo "Choisissez les mod√®les √† installer:"
echo "1. codellama (recommand√© pour le code, ~3.8 GB)"
echo "2. llama3 (mod√®le g√©n√©ral performant, ~4.7 GB)"
echo "3. qwen2.5-coder (excellent pour le code, ~4.7 GB)"
echo "4. deepseek-coder (sp√©cialis√© code, ~3.8 GB)"
echo "5. Tous les mod√®les ci-dessus"
echo "6. Installer un mod√®le personnalis√©"
echo ""

read -p "Votre choix (1-6): " choice

case $choice in
    1)
        install_model "codellama"
        ;;
    2)
        install_model "llama3"
        ;;
    3)
        install_model "qwen2.5-coder"
        ;;
    4)
        install_model "deepseek-coder"
        ;;
    5)
        install_model "codellama"
        install_model "llama3"
        install_model "qwen2.5-coder"
        install_model "deepseek-coder"
        ;;
    6)
        read -p "Nom du mod√®le: " custom_model
        install_model "$custom_model"
        ;;
    *)
        echo "Choix invalide"
        exit 1
        ;;
esac

echo ""
echo "=== Installation termin√©e ==="
echo ""
echo "Mod√®les install√©s:"
docker exec ai-agent-ollama ollama list

echo ""
echo "‚úì Vous pouvez maintenant utiliser l'application!"
echo "  Acc√©dez √† http://localhost:3000"
