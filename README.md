# Agent IA de DÃ©veloppement

Un agent IA qui code vos fonctionnalitÃ©s Ã  la demande. Uploadez votre spÃ©cification fonctionnelle, discutez avec l'agent pour clarifier vos besoins, et gÃ©nÃ©rez automatiquement du code prÃªt Ã  l'emploi.

## ğŸš€ DÃ©marrage Rapide avec Docker (RecommandÃ©)

```bash
# 1. Rendre le script exÃ©cutable
chmod +x start.sh

# 2. DÃ©marrer l'application
./start.sh

# 3. Ouvrir http://localhost:3000
```

**ğŸ“– Guide complet Docker** : Voir [DOCKER.md](DOCKER.md) pour tous les dÃ©tails

## FonctionnalitÃ©s

- ğŸ¤– Agent IA conversationnel basÃ© sur Ollama (100% local)
- ğŸ“„ Upload de spÃ©cifications (TXT, MD, PDF)
- ğŸ’¬ Session de questions/rÃ©ponses interactive
- ğŸ”¨ GÃ©nÃ©ration automatique de code
- ğŸ’¾ Sauvegarde des fichiers gÃ©nÃ©rÃ©s dans un rÃ©pertoire accessible
- ğŸ¨ Interface moderne avec Next.js 14 et shadcn/ui
- ğŸ”Œ Fonctionne en local, mÃªme sans connexion internet (une fois installÃ©)

## PrÃ©requis

### Option 1 : Docker (RecommandÃ© - Plus Simple)

- **Docker** 20.10+ et **Docker Compose** 2.0+
- **8 GB RAM minimum** (16 GB recommandÃ©)
- **10 GB d'espace disque**

Tout est automatisÃ© ! Voir [DOCKER.md](DOCKER.md) pour le guide complet.

### Option 2 : Installation Manuelle

1. **Node.js 18+** - [TÃ©lÃ©charger](https://nodejs.org/)
2. **Ollama** - [TÃ©lÃ©charger](https://ollama.ai/)

### Installation d'Ollama

```bash
# Sur macOS
brew install ollama

# Sur Linux
curl -fsSL https://ollama.ai/install.sh | sh

# Sur Windows
# TÃ©lÃ©charger depuis https://ollama.ai/download
```

### TÃ©lÃ©charger un modÃ¨le Ollama

```bash
# ModÃ¨le recommandÃ© pour le code
ollama pull codellama

# Ou un modÃ¨le gÃ©nÃ©raliste
ollama pull llama3

# Ou DeepSeek Coder (excellent pour le code)
ollama pull deepseek-coder
```

## Installation

```bash
# Cloner ou tÃ©lÃ©charger le projet
cd ai-coding-agent

# Installer les dÃ©pendances
npm install

# DÃ©marrer le serveur de dÃ©veloppement
npm run dev
```

L'application sera accessible sur [http://localhost:3000](http://localhost:3000)

## Utilisation

### 1. DÃ©marrer Ollama

Assurez-vous qu'Ollama est en cours d'exÃ©cution :

```bash
ollama serve
```

Par dÃ©faut, Ollama Ã©coute sur `http://localhost:11434`

### 2. Lancer l'application

```bash
npm run dev
```

### 3. Workflow

1. **Upload SpÃ©cification** : Cliquez sur "Upload SpÃ©cification" et sÃ©lectionnez votre fichier (.txt, .md, .pdf)
2. **Analyse Automatique** : L'agent analyse votre spÃ©cification et pose des questions
3. **Session Q&A** : RÃ©pondez aux questions de l'agent pour clarifier vos besoins
4. **GÃ©nÃ©ration de Code** : Cliquez sur "GÃ©nÃ©rer le Code" quand vous Ãªtes prÃªt
5. **RÃ©cupÃ©ration** : Les fichiers gÃ©nÃ©rÃ©s sont sauvegardÃ©s dans `./output/`

## Structure du Projet

```
ai-coding-agent/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ chat/          # API pour communiquer avec Ollama
â”‚   â”‚   â””â”€â”€ generate-code/ # API pour sauvegarder le code gÃ©nÃ©rÃ©
â”‚   â”œâ”€â”€ globals.css
â”‚   â”œâ”€â”€ layout.tsx
â”‚   â””â”€â”€ page.tsx
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ ui/                # Composants shadcn/ui
â”‚   â””â”€â”€ ai-agent.tsx       # Composant principal de l'agent
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ ollama.ts          # Service Ollama
â”‚   â””â”€â”€ utils.ts
â”œâ”€â”€ output/                # RÃ©pertoire des fichiers gÃ©nÃ©rÃ©s (crÃ©Ã© automatiquement)
â””â”€â”€ package.json
```

## Configuration

### Changer le modÃ¨le Ollama

Vous pouvez sÃ©lectionner diffÃ©rents modÃ¨les directement dans l'interface, ou modifier le modÃ¨le par dÃ©faut dans `lib/ollama.ts` :

```typescript
constructor(baseUrl: string = 'http://localhost:11434', model: string = 'codellama') {
  this.baseUrl = baseUrl;
  this.model = model;
}
```

### ModÃ¨les recommandÃ©s

- **codellama** : Excellent pour la gÃ©nÃ©ration de code (7B-34B)
- **deepseek-coder** : SpÃ©cialisÃ© dans le code, trÃ¨s performant
- **llama3** : Bon Ã©quilibre entre comprÃ©hension et gÃ©nÃ©ration
- **mistral** : Rapide et efficace

### RÃ©pertoire de sortie

Par dÃ©faut, les fichiers sont sauvegardÃ©s dans `./output/`. Pour changer cela, modifiez `app/api/generate-code/route.ts` :

```typescript
const baseDir = path.join(process.cwd(), 'votre-dossier', directory);
```

## Exemples de SpÃ©cifications

### Exemple 1 : Application TODO

```markdown
# SpÃ©cification : Application TODO

## Objectif
CrÃ©er une application de gestion de tÃ¢ches simple

## FonctionnalitÃ©s
- Ajouter une tÃ¢che avec un titre et une description
- Marquer une tÃ¢che comme complÃ©tÃ©e
- Supprimer une tÃ¢che
- Filtrer les tÃ¢ches (toutes, actives, complÃ©tÃ©es)

## Technologie souhaitÃ©e
- Frontend : React
- Stockage : localStorage
```

### Exemple 2 : API REST

```markdown
# SpÃ©cification : API de Blog

## Endpoints requis
- GET /posts - Liste tous les articles
- GET /posts/:id - DÃ©tails d'un article
- POST /posts - CrÃ©er un article
- PUT /posts/:id - Modifier un article
- DELETE /posts/:id - Supprimer un article

## ModÃ¨le de donnÃ©es
- Post : id, title, content, author, createdAt, updatedAt

## Technologie
- Node.js + Express
- Base de donnÃ©es : SQLite
```

## Troubleshooting

### "Ollama non accessible"

1. VÃ©rifiez qu'Ollama est dÃ©marrÃ© : `ollama serve`
2. VÃ©rifiez que le port 11434 est accessible
3. Testez : `curl http://localhost:11434/api/tags`

### "No models available"

```bash
# TÃ©lÃ©charger un modÃ¨le
ollama pull llama3
```

### Erreur de gÃ©nÃ©ration de fichiers

VÃ©rifiez que le rÃ©pertoire `./output/` est accessible en Ã©criture.

## DÃ©veloppement

### Build pour production

```bash
npm run build
npm start
```

### Structure des messages avec Ollama

L'agent utilise un systÃ¨me de messages structurÃ© :

```typescript
{
  role: 'system' | 'user' | 'assistant',
  content: string
}
```

## AmÃ©liorations Futures

- [ ] Support du streaming pour voir le code gÃ©nÃ©rÃ© en temps rÃ©el
- [ ] Historique des sessions sauvegardÃ© localement
- [ ] Export des conversations
- [ ] Preview du code avant sauvegarde
- [ ] Support de plus de formats de spÃ©cifications
- [ ] Tests unitaires intÃ©grÃ©s
- [ ] GÃ©nÃ©ration de documentation automatique
- [ ] Support multi-fichiers avec architecture complÃ¨te

## Licence

MIT

## Auteur

DÃ©veloppÃ© pour Nantares Consulting - Cloud & FinOps Expert
