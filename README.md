# ğŸ¤– Agent IA de DÃ©veloppement

<div align="center">

**Un agent IA qui code vos fonctionnalitÃ©s Ã  la demande**

Uploadez votre spÃ©cification fonctionnelle, discutez avec l'agent pour clarifier vos besoins, et gÃ©nÃ©rez automatiquement du code prÃªt Ã  l'emploi.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Next.js](https://img.shields.io/badge/Next.js-14-black)](https://nextjs.org/)
[![Ollama](https://img.shields.io/badge/Ollama-Local%20AI-blue)](https://ollama.ai/)
[![Docker](https://img.shields.io/badge/Docker-Supported-2496ED)](https://www.docker.com/)

</div>

---

## ğŸ“‘ Table des MatiÃ¨res

- [ğŸš€ DÃ©marrage Rapide](#-dÃ©marrage-rapide)
- [âœ¨ FonctionnalitÃ©s](#-fonctionnalitÃ©s)
- [ğŸ“‹ PrÃ©requis](#-prÃ©requis)
- [ğŸ“¥ Installation](#-installation)
- [ğŸ¯ Utilisation](#-utilisation)
- [ğŸ¤– ModÃ¨les LLM](#-modÃ¨les-llm)
- [ğŸ“ Structure du Projet](#-structure-du-projet)
- [ğŸ”§ Configuration](#-configuration)
- [ğŸ“š Documentation](#-documentation)
- [ğŸ†˜ Support](#-support)

---

## ğŸš€ DÃ©marrage Rapide

### Option 1 : Docker (RecommandÃ©)

```bash
# 1. Rendre le script exÃ©cutable
chmod +x start.sh

# 2. DÃ©marrer l'application
./start.sh

# 3. Ouvrir votre navigateur
# â†’ http://localhost:3000
```

**C'est tout !** ğŸ‰ L'application se lance avec tous les services nÃ©cessaires.

ğŸ“– **Guide dÃ©taillÃ©** : [Documentation/DOCKER-QUICKSTART.md](Documentation/DOCKER-QUICKSTART.md)

### Option 2 : Installation Locale

```bash
# 1. Installer Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# 2. TÃ©lÃ©charger un modÃ¨le
ollama pull qwen2.5-coder:1.5b

# 3. DÃ©marrer Ollama
ollama serve &

# 4. Installer et lancer l'app
npm install
npm run dev
```

ğŸ“– **Guide dÃ©taillÃ©** : [Documentation/QUICKSTART.md](Documentation/QUICKSTART.md)

---

## âœ¨ FonctionnalitÃ©s

### ğŸ¯ Principales

- **ğŸ¤– IA 100% Locale** : BasÃ© sur Ollama, fonctionne sans connexion internet
- **ğŸ“„ Upload de SpÃ©cifications** : Supporte TXT, Markdown et PDF
- **ğŸ’¬ Conversation Interactive** : L'agent pose des questions pour clarifier vos besoins
- **ğŸ”¨ GÃ©nÃ©ration Automatique** : CrÃ©ation de code multi-fichiers prÃªt Ã  l'emploi
- **ğŸ’¾ Gestion de Sessions** : Chaque gÃ©nÃ©ration dans un rÃ©pertoire horodatÃ©
- **ğŸ“¦ Export Complet** : TÃ©lÃ©chargement individuel ou ZIP de tous les fichiers
- **ğŸ¨ Interface Moderne** : Design avec Next.js 14 et shadcn/ui

### ğŸ”§ Techniques

- Architecture Next.js 14 avec App Router
- TypeScript pour la robustesse du code
- Streaming de rÃ©ponses en temps rÃ©el
- SÃ©lection dynamique des modÃ¨les LLM
- Support Docker avec docker-compose
- Healthchecks et monitoring
- Scripts d'administration intÃ©grÃ©s

---

## ğŸ“‹ PrÃ©requis

### Docker (RecommandÃ©)

- **Docker** 20.10+ et **Docker Compose** 2.0+
- **8 GB RAM minimum** (16 GB recommandÃ© pour modÃ¨les avancÃ©s)
- **10 GB d'espace disque** libre
- SystÃ¨me : Linux, macOS, ou Windows avec WSL2

### Installation Manuelle

- **Node.js** 18 ou supÃ©rieur
- **Ollama** installÃ© et fonctionnel
- **6 GB RAM minimum** pour les modÃ¨les lÃ©gers
- SystÃ¨me : Linux, macOS, ou Windows

ğŸ“– **DÃ©tails** : [Documentation/MEMORY-REQUIREMENTS.md](Documentation/MEMORY-REQUIREMENTS.md)

---

## ğŸ“¥ Installation

### Avec Docker (RecommandÃ©)

```bash
# Cloner le projet
git clone <votre-repo>
cd agent-ai

# Rendre le script exÃ©cutable
chmod +x start.sh

# DÃ©marrer tous les services
./start.sh
```

**Services lancÃ©s** :
- ğŸ³ Ollama (port 11434)
- ğŸŒ Application Web (port 3000)
- ğŸ’¾ Volumes persistants pour modÃ¨les et donnÃ©es

### Installation Locale

```bash
# Cloner le projet
git clone <votre-repo>
cd agent-ai

# Installer les dÃ©pendances
npm install

# Configurer l'environnement (optionnel)
cp .env.example .env.local

# DÃ©marrer Ollama
ollama serve &

# TÃ©lÃ©charger le modÃ¨le par dÃ©faut
ollama pull qwen2.5-coder:1.5b

# DÃ©marrer l'application
npm run dev
```

L'application sera accessible sur **http://localhost:3000**

---

## ğŸ¯ Utilisation

### Workflow Complet

```
1. ğŸ“¤ Upload SpÃ©cification
   â†“
2. ğŸ¤– Analyse Automatique par l'IA
   â†“
3. ğŸ’¬ Session Q&A Interactive
   â†“
4. âœ… Validation des Besoins
   â†“
5. ğŸ”¨ GÃ©nÃ©ration du Code
   â†“
6. ğŸ’¾ Sauvegarde Automatique
   â†“
7. ğŸ“¦ TÃ©lÃ©chargement (fichiers individuels ou ZIP)
```

### Ã‰tapes DÃ©taillÃ©es

#### 1. PrÃ©parer votre SpÃ©cification

CrÃ©ez un fichier `.md`, `.txt` ou `.pdf` contenant :

```markdown
# Projet : Application TODO

## Objectif
CrÃ©er une application de gestion de tÃ¢ches

## FonctionnalitÃ©s
- Ajouter/Modifier/Supprimer des tÃ¢ches
- Marquer comme complÃ©tÃ©
- Filtrer par statut

## Technologies
- Frontend : React avec TypeScript
- Styling : Tailwind CSS
- Stockage : localStorage
```

ğŸ“– **Exemples complets** : [examples/spec-calculatrice.md](examples/spec-calculatrice.md)

#### 2. Uploader et Discuter

1. Ouvrez http://localhost:3000
2. Cliquez sur **"Upload SpÃ©cification"**
3. SÃ©lectionnez votre fichier
4. L'agent analyse et pose des questions
5. RÃ©pondez pour clarifier vos besoins

#### 3. GÃ©nÃ©rer le Code

1. Cliquez sur **"GÃ©nÃ©rer le Code"** quand prÃªt
2. Attendez la gÃ©nÃ©ration (quelques minutes selon la complexitÃ©)
3. Les fichiers apparaissent automatiquement

#### 4. TÃ©lÃ©charger

- **Fichier individuel** : Clic sur le bouton de tÃ©lÃ©chargement
- **Tous les fichiers** : Clic sur **"TÃ©lÃ©charger ZIP"**

Les fichiers sont sauvegardÃ©s dans `./output/session_<timestamp>/`

---

## ğŸ¤– ModÃ¨les LLM

### ModÃ¨le Par DÃ©faut

**qwen2.5-coder:1.5b** - Excellent Ã©quilibre vitesse/qualitÃ©
- âš¡ Rapide : ~45s pour une API CRUD
- ğŸ’¾ LÃ©ger : 2-3 GB RAM
- â­ QualitÃ© : 8/10 pour la gÃ©nÃ©ration de code

### Autres ModÃ¨les RecommandÃ©s

| ModÃ¨le | RAM | SpÃ©cialitÃ© | Quand l'utiliser |
|--------|-----|------------|------------------|
| **deepseek-coder:1.3b** | 2 GB | Code | Ultra rapide, machines limitÃ©es |
| **qwen2.5-coder:7b** | 6-8 GB | Code | QualitÃ© supÃ©rieure, code complexe |
| **deepseek-coder:6.7b** | 6-8 GB | Code | Architectures avancÃ©es |
| **codellama:7b** | 6-8 GB | Code | RÃ©fÃ©rence classique Meta |
| **phi3:mini** | 4 GB | Code/Chat | Polyvalent, bon dialogue |

### Installation d'un Nouveau ModÃ¨le

```bash
# Avec Docker
./start.sh pull-model qwen2.5-coder:7b

# Ou directement
docker exec ai-agent-ollama ollama pull deepseek-coder:6.7b

# En local
ollama pull codellama:7b
```

### Changement de ModÃ¨le

- **Via l'interface** : SÃ©lecteur dans la section Configuration
- **Via environnement** : Variable `OLLAMA_MODEL` dans `.env.local` ou `docker-compose.yml`

ğŸ“– **Guide complet des modÃ¨les** : [Documentation/MODELES-LLM.md](Documentation/MODELES-LLM.md)

---

## ğŸ“ Structure du Projet

```
agent-ai/
â”œâ”€â”€ ğŸ“± app/                          # Application Next.js 14
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ chat/                   # API Communication Ollama
â”‚   â”‚   â”œâ”€â”€ generate-code/          # API GÃ©nÃ©ration fichiers
â”‚   â”‚   â”œâ”€â”€ download/               # API TÃ©lÃ©chargement individuel
â”‚   â”‚   â””â”€â”€ download-zip/           # API TÃ©lÃ©chargement ZIP
â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â””â”€â”€ ollama.ts               # Service Ollama
â”‚   â”œâ”€â”€ globals.css
â”‚   â”œâ”€â”€ layout.tsx
â”‚   â””â”€â”€ page.tsx
â”‚
â”œâ”€â”€ ğŸ¨ components/
â”‚   â”œâ”€â”€ ui/                         # Composants shadcn/ui
â”‚   â””â”€â”€ ai-agent.tsx                # Composant principal Agent
â”‚
â”œâ”€â”€ ğŸ“ examples/
â”‚   â””â”€â”€ spec-calculatrice.md        # Exemple de spÃ©cification
â”‚
â”œâ”€â”€ ğŸ’¾ output/                       # Fichiers gÃ©nÃ©rÃ©s (auto-crÃ©Ã©)
â”‚   â””â”€â”€ session_YYYYMMDD_HHMMSS/   # Une session par gÃ©nÃ©ration
â”‚
â”œâ”€â”€ ğŸ“š Documentation/                # ğŸ“– Toute la documentation
â”‚   â”œâ”€â”€ INDEX.md                    # Index complet
â”‚   â”œâ”€â”€ QUICKSTART.md               # DÃ©marrage rapide
â”‚   â”œâ”€â”€ DOCKER-QUICKSTART.md        # DÃ©marrage Docker 3 commandes
â”‚   â”œâ”€â”€ DOCKER.md                   # Guide Docker complet
â”‚   â”œâ”€â”€ DOCKER-VERSIONS.md          # Comparatif des configs Docker
â”‚   â”œâ”€â”€ MODELES-LLM.md             # ğŸ†• Guide complet des modÃ¨les LLM
â”‚   â”œâ”€â”€ TROUBLESHOOTING.md          # DÃ©pannage gÃ©nÃ©ral
â”‚   â”œâ”€â”€ BUILD-TROUBLESHOOTING.md    # ProblÃ¨mes de build
â”‚   â”œâ”€â”€ INSTALL-MODEL.md            # Installation de modÃ¨les
â”‚   â”œâ”€â”€ MEMORY-REQUIREMENTS.md      # Configuration RAM
â”‚   â””â”€â”€ ...                         # Autres guides
â”‚
â”œâ”€â”€ ğŸ³ Docker/
â”‚   â”œâ”€â”€ Dockerfile                  # Build multi-stage optimisÃ©
â”‚   â”œâ”€â”€ docker-compose.yml          # Config production
â”‚   â”œâ”€â”€ docker-compose.simple.yml   # Config dÃ©veloppement
â”‚   â””â”€â”€ docker-compose.nohealth.yml # Config max compatibilitÃ©
â”‚
â”œâ”€â”€ ğŸ”§ Scripts/
â”‚   â”œâ”€â”€ start.sh                    # ğŸ¯ Script principal
â”‚   â”œâ”€â”€ clean-all.sh               # Nettoyage complet
â”‚   â”œâ”€â”€ diagnose.sh                # Diagnostic systÃ¨me
â”‚   â””â”€â”€ check-memory.sh            # VÃ©rification RAM
â”‚
â””â”€â”€ âš™ï¸ Configuration/
    â”œâ”€â”€ package.json
    â”œâ”€â”€ next.config.js
    â”œâ”€â”€ tsconfig.json
    â”œâ”€â”€ tailwind.config.ts
    â””â”€â”€ .gitignore
```

---

## ğŸ”§ Configuration

### Variables d'Environnement

CrÃ©ez un fichier `.env.local` (installation locale) :

```bash
# URL du serveur Ollama
OLLAMA_BASE_URL=http://localhost:11434

# ModÃ¨le Ã  utiliser
OLLAMA_MODEL=qwen2.5-coder:1.5b

# Port de l'application (optionnel)
PORT=3000
```

Pour Docker, modifiez `docker-compose.yml` :

```yaml
services:
  app:
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL=qwen2.5-coder:1.5b
```

### Commandes start.sh

Le script `start.sh` centralise toutes les opÃ©rations :

```bash
./start.sh                      # DÃ©marrer l'application
./start.sh stop                 # ArrÃªter proprement
./start.sh restart              # RedÃ©marrer
./start.sh logs                 # Voir les logs en temps rÃ©el
./start.sh status               # Statut des services
./start.sh pull-model <nom>     # TÃ©lÃ©charger un modÃ¨le
./start.sh list-models          # Lister les modÃ¨les installÃ©s
./start.sh clean                # Nettoyer complÃ¨tement
./start.sh rebuild              # Reconstruire l'app
./start.sh shell                # Shell dans le conteneur app
./start.sh ollama-shell         # Shell dans le conteneur ollama
```

### Personnalisation du ModÃ¨le

**Dans le code** (`app/lib/ollama.ts`) :

```typescript
constructor(baseUrl?: string, model: string = 'votre-modele') {
  this.baseUrl = baseUrl || process.env.OLLAMA_BASE_URL || 'http://localhost:11434';
  this.model = process.env.OLLAMA_MODEL || model;
}
```

---

## ğŸ“š Documentation

### ğŸš€ Pour DÃ©marrer

- **[Documentation/INDEX.md](Documentation/INDEX.md)** - ğŸ“‹ Index complet de toute la documentation
- **[Documentation/QUICKSTART.md](Documentation/QUICKSTART.md)** - âš¡ DÃ©marrage rapide en 5 minutes
- **[Documentation/DOCKER-QUICKSTART.md](Documentation/DOCKER-QUICKSTART.md)** - ğŸ³ Docker en 3 commandes
- **[Documentation/START-HERE.md](Documentation/START-HERE.md)** - ğŸ‘‹ Guide express 2 minutes

### ğŸ³ Docker

- **[Documentation/DOCKER.md](Documentation/DOCKER.md)** - Guide Docker complet
- **[Documentation/DOCKER-VERSIONS.md](Documentation/DOCKER-VERSIONS.md)** - Comparatif des 3 configs Docker

### ğŸ¤– ModÃ¨les LLM

- **[Documentation/MODELES-LLM.md](Documentation/MODELES-LLM.md)** - ğŸ†• Guide exhaustif de tous les modÃ¨les
- **[Documentation/INSTALL-MODEL.md](Documentation/INSTALL-MODEL.md)** - Installation de modÃ¨les
- **[Documentation/MEMORY-REQUIREMENTS.md](Documentation/MEMORY-REQUIREMENTS.md)** - Configuration RAM

### ğŸ› DÃ©pannage

- **[Documentation/TROUBLESHOOTING.md](Documentation/TROUBLESHOOTING.md)** - Guide de dÃ©pannage complet
- **[Documentation/BUILD-TROUBLESHOOTING.md](Documentation/BUILD-TROUBLESHOOTING.md)** - ProblÃ¨mes de build
- **[Documentation/ERROR-500-FIX.md](Documentation/ERROR-500-FIX.md)** - Erreur 500
- **[Documentation/HEALTHCHECK-FIX.md](Documentation/HEALTHCHECK-FIX.md)** - ProblÃ¨mes healthcheck
- **[Documentation/QUICKFIX.md](Documentation/QUICKFIX.md)** - Fixes rapides 30s

### ğŸ“– Guides ComplÃ©mentaires

- **[Documentation/FIXES.md](Documentation/FIXES.md)** - Historique des corrections
- **[Documentation/CLAUDE-CODE-MIGRATION.md](Documentation/CLAUDE-CODE-MIGRATION.md)** - Migration Claude Code

**ğŸ“‹ Conseil** : Commencez par [Documentation/INDEX.md](Documentation/INDEX.md) qui rÃ©fÃ©rence tous les documents !

---

## ğŸ†˜ Support

### En Cas de ProblÃ¨me

1. **Consultez la documentation** : [Documentation/INDEX.md](Documentation/INDEX.md)
2. **Erreur spÃ©cifique** : Voir [Documentation/TROUBLESHOOTING.md](Documentation/TROUBLESHOOTING.md)
3. **Nettoyage complet** : `./clean-all.sh` puis `./start.sh`

### Erreurs Courantes

| SymptÃ´me | Solution Rapide | Documentation |
|----------|-----------------|---------------|
| "Ollama non accessible" | `./start.sh restart` | [DOCKER.md](Documentation/DOCKER.md) |
| "No models available" | `./start.sh pull-model qwen2.5-coder:1.5b` | [INSTALL-MODEL.md](Documentation/INSTALL-MODEL.md) |
| Build Ã©choue | `./clean-all.sh && ./start.sh` | [BUILD-TROUBLESHOOTING.md](Documentation/BUILD-TROUBLESHOOTING.md) |
| Erreur 500 | VÃ©rifier logs : `./start.sh logs` | [ERROR-500-FIX.md](Documentation/ERROR-500-FIX.md) |
| Container unhealthy | Voir healthcheck | [HEALTHCHECK-FIX.md](Documentation/HEALTHCHECK-FIX.md) |

### Diagnostics

```bash
# Diagnostic complet
./diagnose.sh

# VÃ©rifier la mÃ©moire
./check-memory.sh

# Logs dÃ©taillÃ©s
./start.sh logs

# Statut des services
./start.sh status
docker ps -a
```

### Contact

Pour toute question ou support :

**Email** : johan@nantares.consulting
**Projet** : Nantares Consulting - Cloud & FinOps Expert

---

## ğŸš€ DÃ©veloppement

### Build pour Production

```bash
# Build local
npm run build
npm start

# Build Docker
docker-compose -f docker-compose.yml build
docker-compose -f docker-compose.yml up -d
```

### Structure des API

**`POST /api/chat`** - Communiquer avec Ollama
```typescript
{
  messages: Message[],
  model?: string,
  stream?: boolean
}
```

**`POST /api/generate-code`** - Sauvegarder le code gÃ©nÃ©rÃ©
```typescript
{
  code: string,
  filename: string,
  directory: string
}
```

**`GET /api/download?file=<path>`** - TÃ©lÃ©charger un fichier

**`POST /api/download-zip`** - CrÃ©er et tÃ©lÃ©charger un ZIP
```typescript
{
  directory: string
}
```

---

## ğŸ¯ Roadmap

### Version Actuelle (v1.0)
- âœ… Agent conversationnel
- âœ… Upload de spÃ©cifications
- âœ… GÃ©nÃ©ration multi-fichiers
- âœ… TÃ©lÃ©chargement ZIP
- âœ… SÃ©lection de modÃ¨les
- âœ… Support Docker

### Prochaines Versions

#### v1.1 (PlanifiÃ©)
- [ ] Streaming en temps rÃ©el de la gÃ©nÃ©ration
- [ ] Preview du code avant sauvegarde
- [ ] Historique des sessions sauvegardÃ©
- [ ] Export des conversations

#### v1.2 (Futur)
- [ ] Support de plus de formats (DOCX, HTML)
- [ ] Tests unitaires gÃ©nÃ©rÃ©s automatiquement
- [ ] Documentation automatique du code
- [ ] Support multi-projets
- [ ] IntÃ©gration Git automatique

---

## ğŸ“„ Licence

MIT License - Voir [LICENSE](LICENSE) pour plus de dÃ©tails.

Vous Ãªtes libre de :
- âœ… Utiliser commercialement
- âœ… Modifier le code
- âœ… Distribuer
- âœ… Utiliser en privÃ©

---

## ğŸ™ Remerciements

Ce projet utilise et remercie :

- **[Ollama](https://ollama.ai/)** - Pour l'infrastructure LLM locale
- **[Next.js](https://nextjs.org/)** - Framework React
- **[shadcn/ui](https://ui.shadcn.com/)** - Composants UI
- **[Qwen](https://github.com/QwenLM/Qwen2.5)** - ModÃ¨le par dÃ©faut excellent
- **[DeepSeek](https://github.com/deepseek-ai/DeepSeek-Coder)** - ModÃ¨les de code performants

---

## ğŸ“Š Statistiques du Projet

- **Technologies** : Next.js 14, TypeScript, Ollama, Docker
- **Lignes de code** : ~3000+ (app + docs)
- **Documentation** : 20+ guides dÃ©taillÃ©s
- **ModÃ¨les supportÃ©s** : 15+ modÃ¨les LLM
- **Temps de setup** : < 5 minutes avec Docker

---

<div align="center">

**DÃ©veloppÃ© avec â¤ï¸ par Nantares Consulting**

*Cloud & FinOps Expert*

[Documentation](Documentation/INDEX.md) â€¢ [Guide Docker](Documentation/DOCKER.md) â€¢ [ModÃ¨les LLM](Documentation/MODELES-LLM.md) â€¢ [Support](mailto:johan@nantares.consulting)

</div>
