# Configuration m√©moire et choix des mod√®les

## üîç Probl√®me : "model requires more system memory"

Si vous voyez cette erreur :
```
{"error":"model requires more system memory (5.5 GiB) than is available (2.3 GiB)"}
```

Cela signifie que le mod√®le n√©cessite plus de RAM que ce qui est disponible sur votre syst√®me.

## üìä Exigences m√©moire par mod√®le

### Mod√®les ULTRA L√âGERS (< 4 GB RAM disponible requis)

| Mod√®le | RAM requise | Taille t√©l√©chargement | Qualit√© Code | Commande |
|--------|-------------|----------------------|--------------|----------|
| **qwen2.5-coder:1.5b** ‚≠ê | ~2 GB | 900 MB | ‚≠ê‚≠ê‚≠ê‚≠ê | `ollama pull qwen2.5-coder:1.5b` |
| deepseek-coder:1.3b | ~1.5 GB | 800 MB | ‚≠ê‚≠ê‚≠ê‚≠ê | `ollama pull deepseek-coder:1.3b` |
| phi3:mini | ~2.3 GB | 2.3 GB | ‚≠ê‚≠ê‚≠ê‚≠ê | `ollama pull phi3:mini` |
| tinyllama | ~1 GB | 637 MB | ‚≠ê‚≠ê‚≠ê | `ollama pull tinyllama` |
| gemma:2b | ~2.5 GB | 1.7 GB | ‚≠ê‚≠ê‚≠ê‚≠ê | `ollama pull gemma:2b` |

**‚úÖ RECOMMAND√â : qwen2.5-coder:1.5b** - Excellent √©quilibre entre performance et consommation m√©moire

### Mod√®les STANDARDS (>= 6 GB RAM disponible requis)

| Mod√®le | RAM requise | Taille t√©l√©chargement | Qualit√© Code | Commande |
|--------|-------------|----------------------|--------------|----------|
| codellama | ~5.5 GB | 3.8 GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | `ollama pull codellama` |
| llama3 | ~4.7 GB | 4.7 GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | `ollama pull llama3` |
| qwen2.5-coder | ~4.7 GB | 4.7 GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | `ollama pull qwen2.5-coder` |
| deepseek-coder | ~3.8 GB | 3.8 GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | `ollama pull deepseek-coder` |

### Mod√®les QUANTIFI√âS (versions compress√©es)

Pour utiliser des mod√®les standards avec moins de RAM, essayez les versions quantifi√©es :

| Mod√®le | RAM requise | Qualit√© | Commande |
|--------|-------------|---------|----------|
| codellama:7b-code-q4_0 | ~3 GB | ‚≠ê‚≠ê‚≠ê‚≠ê | `ollama pull codellama:7b-code-q4_0` |
| llama3:8b-q4_0 | ~3.2 GB | ‚≠ê‚≠ê‚≠ê‚≠ê | `ollama pull llama3:8b-q4_0` |

## üöÄ Solutions rapides

### Option 1 : Utiliser le script automatique (RECOMMAND√â)

```bash
# D√©tecte automatiquement votre configuration et recommande le bon mod√®le
chmod +x check-memory.sh
./check-memory.sh
```

Le script vous proposera d'installer automatiquement le meilleur mod√®le pour votre syst√®me.

### Option 2 : Installation manuelle du mod√®le recommand√©

```bash
# Installer qwen2.5-coder:1.5b (l√©ger et performant)
docker exec ai-agent-ollama ollama pull qwen2.5-coder:1.5b

# Red√©marrer l'application
docker compose restart app
```

### Option 3 : Augmenter la m√©moire Docker

Si vous voulez utiliser des mod√®les plus puissants :

#### Sur Docker Desktop (Windows/Mac)
1. Ouvrir Docker Desktop
2. Settings ‚Üí Resources ‚Üí Memory
3. Augmenter √† au moins **6 GB** (8 GB recommand√©)
4. Cliquer "Apply & Restart"

#### Sur Linux
Docker utilise directement la RAM syst√®me. Fermez les applications non utilis√©es pour lib√©rer de la m√©moire.

## üîß Configuration du projet

### Mod√®le par d√©faut

Le projet utilise maintenant **qwen2.5-coder:1.5b** par d√©faut, qui n√©cessite seulement ~2 GB de RAM.

Vous pouvez changer le mod√®le via :

1. **Variable d'environnement** (dans `docker-compose.yml`) :
```yaml
environment:
  - OLLAMA_MODEL=qwen2.5-coder:1.5b
```

2. **Interface web** : S√©lecteur de mod√®le √† c√¥t√© du statut Ollama

### Fichiers de configuration

- `app/lib/ollama.ts` : Mod√®le par d√©faut c√¥t√© serveur
- `components/ai-agent.tsx` : Mod√®le par d√©faut c√¥t√© client
- `docker-compose.yml` : Configuration Docker et auto-installation

## üìà Comparaison de performances

### Vitesse de r√©ponse (tokens/sec)

Sur un syst√®me avec 8 GB RAM :

| Mod√®le | Vitesse | Qualit√© | Id√©al pour |
|--------|---------|---------|-----------|
| qwen2.5-coder:1.5b | ‚ö°‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | Syst√®mes limit√©s en RAM |
| deepseek-coder:1.3b | ‚ö°‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | Code Python/JS |
| phi3:mini | ‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | Usage g√©n√©ral |
| codellama | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Projets complexes |
| llama3 | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Meilleure qualit√© |

### Consommation m√©moire en pratique

Mesures r√©elles lors de l'ex√©cution :

| Mod√®le | RAM de base | Pendant g√©n√©ration | Pic maximum |
|--------|-------------|-------------------|-------------|
| qwen2.5-coder:1.5b | 1.2 GB | 1.8 GB | 2.1 GB |
| deepseek-coder:1.3b | 1.0 GB | 1.4 GB | 1.6 GB |
| phi3:mini | 1.8 GB | 2.2 GB | 2.5 GB |
| codellama | 4.2 GB | 5.3 GB | 6.0 GB |
| llama3 | 4.5 GB | 5.5 GB | 6.5 GB |

## üéØ Recommandations par configuration

### Configuration minimale (< 4 GB RAM disponible)
```bash
# Installation recommand√©e
docker exec ai-agent-ollama ollama pull qwen2.5-coder:1.5b
```
‚úÖ Fonctionne bien
‚ö†Ô∏è Qualit√© l√©g√®rement r√©duite mais suffisante pour la plupart des t√¢ches

### Configuration standard (4-8 GB RAM disponible)
```bash
# Vous pouvez utiliser des mod√®les quantifi√©s
docker exec ai-agent-ollama ollama pull codellama:7b-code-q4_0
# OU
docker exec ai-agent-ollama ollama pull phi3:mini
```
‚úÖ Bon √©quilibre performance/qualit√©

### Configuration performante (>= 8 GB RAM disponible)
```bash
# Utilisez les mod√®les complets
docker exec ai-agent-ollama ollama pull codellama
docker exec ai-agent-ollama ollama pull llama3
docker exec ai-agent-ollama ollama pull qwen2.5-coder
```
‚úÖ Meilleure qualit√© de code
‚úÖ Compr√©hension de contexte sup√©rieure

## üõ†Ô∏è Scripts d'aide

### check-memory.sh
D√©tecte votre configuration et recommande le meilleur mod√®le :
```bash
chmod +x check-memory.sh
./check-memory.sh
```

### install-models.sh
Menu interactif pour installer des mod√®les :
```bash
chmod +x install-models.sh
./install-models.sh
```

### test-ollama.sh
Teste la connexion et les mod√®les install√©s :
```bash
chmod +x test-ollama.sh
./test-ollama.sh
```

## ‚ùì FAQ

### Q: Puis-je installer plusieurs mod√®les ?
**R:** Oui ! Installez autant de mod√®les que vous voulez, puis s√©lectionnez celui √† utiliser dans l'interface.

### Q: Comment lib√©rer de l'espace disque ?
**R:**
```bash
# Lister les mod√®les
docker exec ai-agent-ollama ollama list

# Supprimer un mod√®le
docker exec ai-agent-ollama ollama rm <nom-du-modele>
```

### Q: Le mod√®le est lent, que faire ?
**R:**
1. Utilisez un mod√®le plus l√©ger (qwen2.5-coder:1.5b)
2. Fermez les applications non utilis√©es
3. Augmentez la RAM Docker si possible

### Q: Quel est le meilleur mod√®le pour le code ?
**R:**
- **Budget RAM limit√©** : qwen2.5-coder:1.5b ou deepseek-coder:1.3b
- **RAM suffisante** : codellama ou qwen2.5-coder (version compl√®te)

### Q: Comment changer le mod√®le par d√©faut ?
**R:**
1. Modifiez `OLLAMA_MODEL` dans `docker-compose.yml`
2. Ou s√©lectionnez un autre mod√®le dans l'interface web

## üìû Support

Si vous rencontrez toujours des probl√®mes :

1. V√©rifiez les logs : `docker compose logs app ollama`
2. Consultez [FIXING-404-ERROR.md](./FIXING-404-ERROR.md)
3. Utilisez le script de diagnostic : `./test-ollama.sh`

## üîÑ Mise √† jour

Pour mettre √† jour vers la configuration optimis√©e pour la m√©moire :

```bash
# Arr√™ter les conteneurs
docker compose down

# Reconstruire avec la nouvelle configuration
docker compose build --no-cache

# D√©marrer avec le nouveau mod√®le par d√©faut
docker compose up -d

# Le mod√®le qwen2.5-coder:1.5b sera automatiquement t√©l√©charg√©
```

C'est tout ! Votre application devrait maintenant fonctionner m√™me sur des syst√®mes avec peu de RAM. üéâ
