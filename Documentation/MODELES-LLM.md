# ü§ñ Guide des Mod√®les LLM - Agent IA de D√©veloppement

Ce document r√©pertorie tous les mod√®les de langage (LLM) que vous pouvez utiliser avec l'Agent IA de D√©veloppement, leurs caract√©ristiques, et comment les utiliser.

## üìã Tableau R√©capitulatif des Mod√®les

### Mod√®les Recommand√©s pour le Code

| Mod√®le | Taille | RAM Requis | Sp√©cialit√© | Performance Code | Vitesse | Recommandation |
|--------|--------|------------|------------|------------------|---------|----------------|
| **qwen2.5-coder:1.5b** | 1.5B | 2-3 GB | Code | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö°‚ö°‚ö° | ‚úÖ **D√©faut - Excellent √©quilibre** |
| **qwen2.5-coder:7b** | 7B | 6-8 GB | Code | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö°‚ö° | ‚úÖ Recommand√© si assez de RAM |
| **deepseek-coder:1.3b** | 1.3B | 2 GB | Code | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö°‚ö°‚ö° | ‚úÖ Ultra rapide, l√©ger |
| **deepseek-coder:6.7b** | 6.7B | 6-8 GB | Code | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö°‚ö° | ‚úÖ Excellent pour code complexe |
| **codellama:7b** | 7B | 6-8 GB | Code | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö° | ‚úÖ Bon choix classique |
| **codellama:13b** | 13B | 12-16 GB | Code | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö° | ‚ö†Ô∏è N√©cessite beaucoup de RAM |
| **starcoder2:3b** | 3B | 3-4 GB | Code | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö°‚ö° | ‚úÖ Bon compromis |
| **phi3:mini** | 3.8B | 4 GB | Code/Chat | ‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö°‚ö° | ‚úÖ Polyvalent |

### Mod√®les G√©n√©ralistes (Usage Avanc√©)

| Mod√®le | Taille | RAM Requis | Sp√©cialit√© | Performance Code | Vitesse | Recommandation |
|--------|--------|------------|------------|------------------|---------|----------------|
| **llama3:8b** | 8B | 8 GB | G√©n√©raliste | ‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö° | ‚úÖ Bon pour dialogue + code |
| **llama3:70b** | 70B | 64+ GB | G√©n√©raliste | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö° | ‚ùå N√©cessite GPU puissant |
| **mistral:7b** | 7B | 6-8 GB | G√©n√©raliste | ‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö°‚ö° | ‚úÖ Rapide et efficace |
| **mixtral:8x7b** | 47B | 32+ GB | G√©n√©raliste | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö° | ‚ö†Ô∏è Tr√®s performant mais lourd |
| **gemma2:2b** | 2B | 2-3 GB | G√©n√©raliste | ‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö°‚ö°‚ö° | ‚úÖ Ultra l√©ger |

### Mod√®les Tr√®s L√©gers (Machines Limit√©es)

| Mod√®le | Taille | RAM Requis | Sp√©cialit√© | Performance Code | Vitesse | Recommandation |
|--------|--------|------------|------------|------------------|---------|----------------|
| **tinyllama** | 1.1B | 1-2 GB | G√©n√©raliste | ‚≠ê‚≠ê | ‚ö°‚ö°‚ö°‚ö°‚ö° | ‚ö†Ô∏è Basique, pour tests |
| **phi3:mini** | 3.8B | 4 GB | Code/Chat | ‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö°‚ö° | ‚úÖ Bon compromis l√©ger |
| **stablelm2:1.6b** | 1.6B | 2 GB | G√©n√©raliste | ‚≠ê‚≠ê | ‚ö°‚ö°‚ö°‚ö°‚ö° | ‚ö†Ô∏è Performances limit√©es |

## üéØ Quel Mod√®le Choisir ?

### Selon Votre Configuration Mat√©rielle

#### üíª Configuration Minimale (4-8 GB RAM)
```bash
# Mod√®le par d√©faut - Excellent choix
ollama pull qwen2.5-coder:1.5b

# Alternative ultra l√©g√®re
ollama pull deepseek-coder:1.3b

# Alternative polyvalente
ollama pull phi3:mini
```

**Recommandation** : `qwen2.5-coder:1.5b` (d√©j√† configur√© par d√©faut)

#### üñ•Ô∏è Configuration Standard (8-16 GB RAM)
```bash
# Meilleur choix pour le code
ollama pull qwen2.5-coder:7b

# Ou DeepSeek pour code complexe
ollama pull deepseek-coder:6.7b

# Ou CodeLlama classique
ollama pull codellama:7b
```

**Recommandation** : `qwen2.5-coder:7b` ou `deepseek-coder:6.7b`

#### üöÄ Configuration Puissante (16+ GB RAM)
```bash
# Pour g√©n√©ration de code avanc√©e
ollama pull codellama:13b

# Pour code + dialogue naturel
ollama pull llama3:70b

# Pour performances maximales
ollama pull mixtral:8x7b
```

**Recommandation** : `codellama:13b` pour le code, `mixtral:8x7b` pour polyvalence

### Selon Votre Cas d'Usage

#### üîß G√©n√©ration de Code Simple (API, CRUD, Scripts)
- **qwen2.5-coder:1.5b** - Rapide, l√©ger, excellent
- **deepseek-coder:1.3b** - Ultra rapide
- **starcoder2:3b** - Bon √©quilibre

#### üèóÔ∏è G√©n√©ration de Code Complexe (Architectures, Patterns)
- **qwen2.5-coder:7b** - Recommand√©
- **deepseek-coder:6.7b** - Tr√®s performant
- **codellama:13b** - Maximum de qualit√©

#### üí¨ Dialogue + Code (Q&A + G√©n√©ration)
- **phi3:mini** - Polyvalent et l√©ger
- **llama3:8b** - Excellent pour conversation
- **mistral:7b** - Rapide et efficace

#### ‚ö° Vitesse Maximale (Prototypage Rapide)
- **deepseek-coder:1.3b** - Le plus rapide
- **qwen2.5-coder:1.5b** - Excellent compromis
- **gemma2:2b** - Tr√®s l√©ger

## üì• Installation des Mod√®les

### Dans Docker (Recommand√©)

```bash
# Lister les mod√®les disponibles
./start.sh list-models

# T√©l√©charger un nouveau mod√®le
./start.sh pull-model qwen2.5-coder:7b

# Ou directement via docker exec
docker exec ai-agent-ollama ollama pull deepseek-coder:6.7b
```

### Installation Locale (Sans Docker)

```bash
# T√©l√©charger un mod√®le
ollama pull qwen2.5-coder:1.5b

# Lister les mod√®les install√©s
ollama list

# Tester un mod√®le
ollama run qwen2.5-coder:1.5b "Write a hello world in Python"
```

## üîÑ Changer de Mod√®le dans l'Application

### Via l'Interface Web
1. Ouvrez http://localhost:3000
2. Dans la section "Configuration", s√©lectionnez le mod√®le souhait√©
3. Le changement est imm√©diat

### Via Variables d'Environnement

**Docker Compose** (`docker-compose.yml`) :
```yaml
services:
  app:
    environment:
      - OLLAMA_MODEL=qwen2.5-coder:7b
```

**Installation Locale** (`.env.local`) :
```bash
OLLAMA_MODEL=deepseek-coder:6.7b
OLLAMA_BASE_URL=http://localhost:11434
```

### Via le Code

Modifiez `app/lib/ollama.ts` :
```typescript
constructor(baseUrl?: string, model: string = 'votre-modele-ici') {
  // ...
}
```

## üìä Comparaison D√©taill√©e des Meilleurs Mod√®les

### Qwen 2.5 Coder (Recommand√© par D√©faut)

**Versions** : 1.5b, 7b, 14b, 32b

**Points Forts** :
- ‚úÖ Excellent pour la g√©n√©ration de code
- ‚úÖ Tr√®s bon √©quilibre vitesse/qualit√©
- ‚úÖ Comprend bien les sp√©cifications
- ‚úÖ Multi-langages (Python, JS, Java, etc.)
- ‚úÖ Bonne gestion du contexte

**Points Faibles** :
- ‚ö†Ô∏è Peut √™tre verbeux dans les r√©ponses
- ‚ö†Ô∏è Version 1.5b limit√©e pour code tr√®s complexe

**Utilisation** :
```bash
# Version l√©g√®re (d√©faut)
ollama pull qwen2.5-coder:1.5b

# Version standard
ollama pull qwen2.5-coder:7b
```

### DeepSeek Coder

**Versions** : 1.3b, 6.7b, 33b

**Points Forts** :
- ‚úÖ Sp√©cialis√© pour le code
- ‚úÖ Excellente compr√©hension des patterns
- ‚úÖ Tr√®s rapide m√™me sur mod√®les l√©gers
- ‚úÖ Bon pour code complexe et architectures
- ‚úÖ G√®re bien les bugs et refactoring

**Points Faibles** :
- ‚ö†Ô∏è Moins bon pour le dialogue naturel
- ‚ö†Ô∏è Peut √™tre trop technique

**Utilisation** :
```bash
# Ultra l√©ger
ollama pull deepseek-coder:1.3b

# Standard
ollama pull deepseek-coder:6.7b
```

### CodeLlama (Meta)

**Versions** : 7b, 13b, 34b, 70b

**Points Forts** :
- ‚úÖ Mod√®le de r√©f√©rence pour le code
- ‚úÖ Excellent pour Python
- ‚úÖ Bonne documentation du code
- ‚úÖ Comprend les instructions complexes
- ‚úÖ Stable et fiable

**Points Faibles** :
- ‚ö†Ô∏è Plus lent que Qwen ou DeepSeek
- ‚ö†Ô∏è N√©cessite plus de RAM
- ‚ö†Ô∏è Peut g√©n√©rer du code verbeux

**Utilisation** :
```bash
# Version standard
ollama pull codellama:7b

# Version puissante
ollama pull codellama:13b
```

### StarCoder2

**Versions** : 3b, 7b, 15b

**Points Forts** :
- ‚úÖ Entra√Æn√© sur √©norm√©ment de code GitHub
- ‚úÖ Excellent pour multiple langages
- ‚úÖ Bon pour les patterns modernes
- ‚úÖ Comprend les frameworks populaires

**Points Faibles** :
- ‚ö†Ô∏è Moins bon pour les sp√©cifications en fran√ßais
- ‚ö†Ô∏è Peut g√©n√©rer du code non idiomatique

**Utilisation** :
```bash
ollama pull starcoder2:3b
ollama pull starcoder2:7b
```

## üîß Configuration Avanc√©e

### Optimiser les Performances

#### GPU NVIDIA
```yaml
# docker-compose.yml
services:
  ollama:
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
```

#### CPU Uniquement
```bash
# Limiter les threads pour √©viter la surchauffe
docker exec ai-agent-ollama ollama run qwen2.5-coder:1.5b --num-thread 4
```

### Param√®tres de G√©n√©ration

Modifiez `app/lib/ollama.ts` pour ajuster :

```typescript
const response = await fetch(`${this.baseUrl}/api/chat`, {
  method: 'POST',
  body: JSON.stringify({
    model: this.model,
    messages: messages,
    stream: false,
    options: {
      temperature: 0.7,      // Cr√©ativit√© (0.0-1.0)
      top_p: 0.9,           // Diversit√© des r√©ponses
      num_predict: 2048,    // Longueur max de r√©ponse
      stop: ["```\n\n"],    // Tokens d'arr√™t
    }
  }),
});
```

## üìà Benchmarks (G√©n√©ration de Code)

### Test : G√©n√©rer une API REST CRUD compl√®te

| Mod√®le | Temps | Qualit√© | RAM Utilis√©e | Note Globale |
|--------|-------|---------|--------------|--------------|
| qwen2.5-coder:1.5b | 45s | 8/10 | 2.1 GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| deepseek-coder:1.3b | 38s | 7.5/10 | 1.8 GB | ‚≠ê‚≠ê‚≠ê‚≠ê |
| qwen2.5-coder:7b | 2m10s | 9.5/10 | 7.2 GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| deepseek-coder:6.7b | 2m05s | 9/10 | 6.8 GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| codellama:7b | 2m45s | 8.5/10 | 7.5 GB | ‚≠ê‚≠ê‚≠ê‚≠ê |
| phi3:mini | 1m15s | 7/10 | 3.9 GB | ‚≠ê‚≠ê‚≠ê |
| llama3:8b | 2m30s | 7.5/10 | 8.1 GB | ‚≠ê‚≠ê‚≠ê |

**Crit√®res** : Temps sur CPU i7 12th gen, Qualit√© = code fonctionnel + bonnes pratiques

## üéì Conseils d'Utilisation

### Pour D√©butants
1. Commencez avec **qwen2.5-coder:1.5b** (d√©j√† install√©)
2. Testez sur des projets simples
3. Si satisfait, restez avec ce mod√®le
4. Si besoin de plus : passez √† **qwen2.5-coder:7b**

### Pour Utilisateurs Avanc√©s
1. **deepseek-coder:6.7b** pour code complexe
2. **codellama:13b** pour projets critiques
3. **mixtral:8x7b** si besoin de polyvalence
4. Testez plusieurs mod√®les et comparez

### Pour Machines Limit√©es
1. **deepseek-coder:1.3b** - Le plus rapide
2. **qwen2.5-coder:1.5b** - Meilleur √©quilibre
3. **phi3:mini** - Si besoin de dialogue
4. √âvitez les mod√®les > 7b

## üÜò D√©pannage

### Le mod√®le ne se charge pas
```bash
# V√©rifier les mod√®les install√©s
docker exec ai-agent-ollama ollama list

# T√©l√©charger le mod√®le
docker exec ai-agent-ollama ollama pull qwen2.5-coder:1.5b

# V√©rifier les logs
docker logs ai-agent-ollama
```

### Erreur de m√©moire (OOM)
```bash
# Utiliser un mod√®le plus l√©ger
docker exec ai-agent-ollama ollama pull qwen2.5-coder:1.5b

# Ou augmenter la RAM Docker
# Docker Desktop > Settings > Resources > Memory
```

### Mod√®le trop lent
```bash
# Passer √† un mod√®le plus l√©ger
./start.sh pull-model deepseek-coder:1.3b

# Ou limiter le contexte dans l'app
# R√©duire la longueur des sp√©cifications
```

## üìö Ressources Suppl√©mentaires

- **Ollama Models Library** : https://ollama.ai/library
- **Qwen Documentation** : https://github.com/QwenLM/Qwen2.5
- **DeepSeek Coder** : https://github.com/deepseek-ai/DeepSeek-Coder
- **CodeLlama Paper** : https://arxiv.org/abs/2308.12950

## üîÑ Mises √† Jour

Les mod√®les sont r√©guli√®rement mis √† jour par leurs cr√©ateurs :

```bash
# Mettre √† jour un mod√®le
docker exec ai-agent-ollama ollama pull qwen2.5-coder:1.5b

# Supprimer une ancienne version
docker exec ai-agent-ollama ollama rm qwen2.5-coder:old-version
```

---

**üí° Conseil Final** : Pour 95% des cas, **qwen2.5-coder:1.5b** (mod√®le par d√©faut) est parfait. N'installez des mod√®les plus lourds que si vous avez un besoin sp√©cifique de qualit√© sup√©rieure.

**Contact** : johan@nantares.consulting
