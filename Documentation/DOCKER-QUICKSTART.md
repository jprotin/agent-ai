# üöÄ D√©marrage Ultra-Rapide avec Docker

## Installation en 3 commandes

```bash
# 1. Rendre le script ex√©cutable
chmod +x start.sh

# 2. D√©marrer tout
./start.sh

# 3. Ouvrir http://localhost:3000
```

C'est tout ! üéâ

## Ou avec Make (encore plus simple)

```bash
make install
```

---

## Ce qui se passe en arri√®re-plan

1. ‚úÖ Docker t√©l√©charge l'image Ollama (~1.5 GB)
2. ‚úÖ Build de l'application Next.js
3. ‚úÖ D√©marrage d'Ollama sur le port 11434
4. ‚úÖ T√©l√©chargement automatique du mod√®le `codellama` (~4 GB)
5. ‚úÖ D√©marrage de l'app sur le port 3000

**Dur√©e totale** : 5-15 minutes selon votre connexion internet

---

## Commandes Essentielles

```bash
# Voir les logs
./start.sh logs
# ou
make logs

# Arr√™ter
./start.sh stop
# ou
make stop

# Red√©marrer
./start.sh restart
# ou
make restart

# T√©l√©charger un autre mod√®le
./start.sh pull-model llama3
# ou
make pull-model MODEL=llama3

# Tout nettoyer
./start.sh clean
# ou
make clean
```

---

## V√©rifications

### ‚úÖ Ollama fonctionne ?
```bash
curl http://localhost:11434/api/tags
```

### ‚úÖ L'app fonctionne ?
```bash
curl http://localhost:3000
```

### ‚úÖ Voir le statut
```bash
./start.sh status
# ou
make status
```

---

## Probl√®mes ?

### Port d√©j√† utilis√©
```bash
# Modifier docker-compose.yml ligne 11
# Changer "3000:3000" en "8080:3000"
```

### Manque de m√©moire
```bash
# Utiliser un mod√®le plus l√©ger
./start.sh pull-model codellama:7b
```

### Ollama ne d√©marre pas
```bash
./start.sh logs ollama
```

---

## Aide Compl√®te

- **Guide Docker complet** : [DOCKER.md](DOCKER.md)
- **Guide g√©n√©ral** : [README.md](README.md)
- **D√©marrage rapide** : [QUICKSTART.md](QUICKSTART.md)

---

## Support

Questions ? johan@nantares.consulting
